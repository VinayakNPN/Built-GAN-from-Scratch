# ğŸ§  GAN from Scratch - MNIST Digit Generator

![Visitors](https://visitor-badge.laobi.icu/badge?page_id=gan-from-scratch)
[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **ğŸš€ I built a GenAI model from scratch** - A complete implementation of Generative Adversarial Networks (GANs) for generating handwritten digits using the MNIST dataset.

## ğŸ“– What are GANs?

**Generative Adversarial Networks (GANs)** are a revolutionary class of machine learning models introduced by Ian Goodfellow in 2014. They consist of two neural networks competing against each other in a zero-sum game framework:


### ğŸ­ The Two Players

#### ğŸ¨ **Generator**
- **Role**: Creates fake data that mimics real data
- **Goal**: Fool the discriminator into thinking generated data is real
- **Input**: Random noise vector (latent space)
- **Output**: Synthetic data (in our case, 28x28 digit images)

#### ğŸ•µï¸ **Discriminator**
- **Role**: Distinguishes between real and fake data
- **Goal**: Correctly classify real vs generated data
- **Input**: Real or generated images
- **Output**: Probability score (0 = fake, 1 = real)

### âš”ï¸ The Adversarial Process

The training process resembles a **cat-and-mouse game**:

1. **Generator** creates fake images from random noise
2. **Discriminator** evaluates both real MNIST images and generated fakes
3. Both networks improve through backpropagation:
   - Generator learns to create more realistic images
   - Discriminator becomes better at spotting fakes
4. Eventually, the Generator becomes so good that the Discriminator can't tell the difference!

![GAN Training Process](https://developers.google.com/static/machine-learning/gan/images/gan_diagram.svg)

## ğŸ—ï¸ Architecture Details

### Generator Network
```
Input: 100D noise vector
â”œâ”€â”€ Dense(256) + LeakyReLU + BatchNorm
â”œâ”€â”€ Dense(512) + LeakyReLU + BatchNorm  
â”œâ”€â”€ Dense(1024) + LeakyReLU + BatchNorm
â”œâ”€â”€ Dense(784) + Tanh
â””â”€â”€ Reshape(28, 28, 1)
```

### Discriminator Network
```
Input: 28x28x1 image
â”œâ”€â”€ Flatten(784)
â”œâ”€â”€ Dense(512) + LeakyReLU
â”œâ”€â”€ Dense(256) + LeakyReLU
â””â”€â”€ Dense(1) + Sigmoid
```

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install tensorflow numpy matplotlib
```


### ğŸ¯ Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/gan-from-scratch.git
cd gan-from-scratch
```

2. **Install dependencies**
```bash
pip install tensorflow numpy matplotlib
```

3. **Run the notebook**
```bash
jupyter notebook gan.ipynb
```

4. **Watch the magic happen!** âœ¨
   - The model will start generating images every 10 epochs
   - Images are saved as `gan_images_epoch_{epoch}.png`

## ğŸ“Š Training Process

The model trains for **10,000 epochs** with the following hyperparameters:
- **Batch Size**: 64
- **Learning Rate**: 0.0002 (Adam optimizer)
- **Beta1**: 0.5
- **Latent Dimension**: 100

### ğŸ“ˆ Loss Functions
- **Generator Loss**: Binary crossentropy (tries to fool discriminator)
- **Discriminator Loss**: Binary crossentropy (real vs fake classification)

## ğŸ¨ Results

As training progresses, you'll see the generated digits evolve from random noise to recognizable handwritten digits:

**Epoch 0** â†’ **Epoch 1000** â†’ **Epoch 5000** â†’ **Epoch 10000**

*Random noise* â†’ *Blurry shapes* â†’ *Digit-like forms* â†’ *Realistic digits*

## ğŸ”§ Key Features

- âœ… **Built from scratch** using TensorFlow/Keras
- âœ… **Well-documented** code with clear explanations
- âœ… **Visualization** of training progress
- âœ… **Modular design** for easy experimentation
- âœ… **MNIST dataset** for quick testing and validation

## ğŸ§ª Experiments & Modifications

Want to experiment? Try these modifications:

1. **Change the latent dimension** (currently 100)
2. **Modify network architectures** (add/remove layers)
3. **Adjust hyperparameters** (learning rate, batch size)
4. **Try different datasets** (CIFAR-10, CelebA)
5. **Implement different GAN variants** (DCGAN, WGAN, etc.)

## ğŸ“š Understanding the Code

### Key Components

1. **Data Preprocessing**: MNIST images normalized to [-1, 1] range
2. **Generator**: Transforms noise into realistic images
3. **Discriminator**: Binary classifier for real vs fake
4. **Adversarial Training**: Alternating training of both networks
5. **Visualization**: Periodic image generation for progress tracking

### ğŸ” Training Loop Breakdown
```python
for epoch in range(epochs):
    # Train Discriminator
    real_images = sample_real_batch()
    fake_images = generator.predict(noise)
    d_loss = train_discriminator(real_images, fake_images)
    
    # Train Generator
    g_loss = train_generator(noise)
    
    # Visualize progress
    if epoch % 10 == 0:
        generate_and_save_images()
```


## ğŸ™ Acknowledgments

- **Ian Goodfellow** for inventing GANs
- **TensorFlow team** for the amazing framework
- **MNIST dataset** creators for the classic benchmark
- **Open source community** for inspiration and resources


**â­ Star this repo if you found it helpful!**

*Built with â¤ï¸ and by Vinayak Chouhan â˜•*
